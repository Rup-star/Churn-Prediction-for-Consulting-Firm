# Run in Azure Databricks Notebook

# Import the necessary libraries
from pyspark.sql import SparkSession
from data_pipeline import load_data, preprocess_data

# Load and preprocess data
data = load_data("/dbfs/data/client_data.csv")
processed_data = preprocess_data(data)

# Show the processed data
processed_data.show()

# Save the processed data back to storage
processed_data.write.csv("/dbfs/data/processed_data.csv", header=True)
